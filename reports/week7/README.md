# Towards Resiliency

After Day 24, I decided to have a short break from programming, and instead read up on the topic of Microservices and Sagas in general and resiliency in general due to the following issues the current solution has:
- There is a potential for processing the same Domain Event multiple times, so I need some form of dedup mechanism (see https://lnkd.in/dmFC28qc)
- There is an edge case where committing the DB TX goes through but the Kafka committing fails for whatever reason - in this case we would lose the Domain Event, potentially leading to inconsistencies.
- My current Saga implementation when committing Referee Assignments is not robust and results in inconsistencies in case of infrastructure failures.

I focussed on the following reading/blog/video materials:
- "Building Microservices, 2nd Edition" by Sam Newman. More high-level and conceptual.
- "Microservices Patterns" by Chris Richardson. Very technical, goes into the dirty implementation details.
- [Life beyond Distributed Transactions by Pat Helland](https://queue.acm.org/detail.cfm?id=3025012) - given its a very difficult but fundamentally important article on this topic, I recommend this youtube video makes things clearer: https://www.youtube.com/watch?v=AUrKofVRHV4
- [The Limits of the Saga Pattern](https://www.ufried.com/blog/limits_of_saga_pattern/#:~:text=The%20Saga%20pattern%20can%20only,existing%20business%20rule%20is%20violated)

Below I am reflecting on the main takeaways from the readings and how they can help me to address the issues above.


Sam Newmann makes one thing fundamentally clear about Sagas in his book "Building Microservices" (Second Edition, Chapter 6: Workflow, page 185)

> It's really important to note that a saga allows us to recover from BUSINESS failures, not TECHNICAL failures. For example, if we try and take payment from the customer but the customer has insufficient funds, then this is a business failure that the saga should be expected to handle. On the other hand, if the Payment services times out or throws a '500 Internal Service Error', then this is technical failure that we need to handle separately. The saga assumes the underlying components are working properly - that the underlying system is reliable, and that we are then coordinating the work of reliable components.* -

This is also expressed in the blog post [The Limits of the Saga Pattern](https://www.ufried.com/blog/limits_of_saga_pattern/#:~:text=The%20Saga%20pattern%20can%20only,existing%20business%20rule%20is%20violated) by Uwe Friedrichsen: 

> The Saga pattern can only be used to logically roll back transactions due to business errors. The Saga pattern cannot be used to respond to technical errors.

> A business error occurs if an existing business rule is violated. E.g., if you try to pay with an expired credit card or if a pending payment would exceed your credit limit: That is a business error. From a technical point of view, everything is fine. A technical error on the other hand is if something goes wrong on the technology, the infrastructure level. E.g., if your database does not respond or throws an unexpected technical error (the dreaded “IOException”), if a service is down or latent, if a message is lost or corrupted, if your replicated and eventually consistent database is out of sync: All that are technical errors. From a business point of view, everything is fine.

> Technical errors in a distributed system landscape are non-deterministic 1. You cannot predict when they will occur and how they will manifest. The only thing you know for sure is that they will happen, i.e., their probability is bigger than 0.

> Thus, if you try to respond to technical errors with a one-shot, deterministic approach like the Saga pattern, chances are that your compensating action will also fall prey to another unexpected technical error.

> In practice this means if you naively apply the Saga pattern also to respond to technical errors, you will eventually end up with an inconsistent database – guaranteed!

> This leaves the question: How can we deal with technical errors? In general, the only way is to strive for eventual completion. If you face a technical error, you need to retry in some (non-naive) way until you eventually overcome the error and your activity succeeds. This means retrying in combination with waiting, escalation strategies, etc.

A very difficult but fundamentally important article on this topic is [Life beyond Distributed Transactions by Pat Helland](https://queue.acm.org/detail.cfm?id=3025012) - given its a very difficult I recommend this youtube video makes things clearer: https://www.youtube.com/watch?v=AUrKofVRHV4. It's main takeaways are these:
- *Assume a transactional boundary of a single item.* This matches perfectly with the idea of microservices where we cut by aggregates, that constitute a transactional boundary around one or multiple entities. Across a such a single Aggregate (Entity), we can and should guaranteed a transactional boundary, that runs on a **local** database (remember, each microservice has its own database). It is important to note that it is speaking aobut a **single** item. For example in the project we have a `Referee`  aggregate that is managed by the referees service: updating the club of the referee happens on a single item inside a clearly defined local transactional boundary. On the other hand, when committing assignments of referees to fixtures, there are 2 items (entities) involved: the `Assignment` and `Fixture` aggregate: the `Assignment` stores temporary assignments of refereees to a fixture, so that they can be changed by an admin to eventually reach a global state that makes the admin happy - note that this is a **business** state, and has nothing to do with eventual consistency on a technical level. Now, when the admin is happy it wants to move the `Assignment` to the `Fixture` aggregate, however this means we would need a transaction that spans two items (entities) and thus two different transactional boundaries - this is a **business** transaction, and not a technical one. Given that they both live in different services, and therefore databases (which is precisely the point of microservices), we cannot enforce this transaction across the two services (no, distributed transactions are explicitly not allowed). The way we solve this problem is by publishing a Domain Event from the assignments service that the fixtures service listens to and updates the corresponding fixture. Now this means there is an eventual-consistent business flow involved. Also, it could be the case that writing the assigned referee to the Fixture in the fixtures services violates some buisness rule (e.g. there was already some referee assigned), so this must be handled via some alternate business flow - the business flow part is not the hard part in this whole thing. The hard part is to make sure that: 1. the domain event is emitted **at least once**, 2. the domain event is processed **exactly once**, 3. re-ordering of the domain event doesn't cause any harm.
To address the 1st problem we store the Domain Event that is about to be emitted from the assignments service in the assignments services transactional boundary and employ some mechanism that emits the message. Why do we need to store it in the services transactional boundary? The reason is simply the fact that we need some means of also capturing the domain event emitted in a transactional boundary, otherwise we might risk losing it: imagine we commit the DB transaction in the assignments service that updates an `Assignment` aggregate to *committed* (or *pending*) state (the actual state is not important, just that some business-related state change happened, that needs to trigger another state change in another entity) and then emit the Domain Event (e.g. via Kafka). Both of these operations are transactionally independent from each other (yes, even if we use Kafka transactions, because Kafka Transactions have nothing to do with DB transactions), which means we could end up with the situation where the DB transaction successfully commits, but emitting the domain event fails due to some infrastructure error (which is always possible in a distributed system due to partitioning). In this case we would lose the domain event and end up in an inconsistent state from a business perspective - something we have to avoid at all costs. The solution is therefore to have some *DomainEvent* table that can be seen as an *outbox* of all Domain Events to emit. Then to emit a Domain Event we do not emit them directly via a messaging service, but write it to the *DomainEvent* table inside the SAME transaction where we update the `Assignment` aggregate. This way we make sure we don't "forget" our Domain Event in case it cant be emitted - because emitting is now a DB insertion, and if this one fails, the whole TX fails, therefore the state change of the `Assignment` aggregate fails as well, and we stay consistent business-wise.
How can we emit the Domain Events from the *DomainEvent* table? There are different ways to do this: the simplest is to implement an asynchronous processor that either gets notified via some DB mechanism that the table has changed and fetches the next Domain Event to emit and sends it via Kafka - or you are lucky that your ecosystem has some advanced libraries that support this out of the box. (TODO: afair the Microservices patterns book speaks about this). However ther is still a very subtle problem: you still have two different transactional boundaries: one that updates the *DomainEvent* table, marking the just emitted Domain Event as *emitted* and the messaging boundary. This time however what we do is we emit the event from within the DB transactional boundary: if emitting the event fails we would also fail the DB transaction and nothing would be updated in the DB - another option would be to retry. We could use Kafka transactions to improve performance here and emit Domain Events in a "batched" fashion (which would become only relevant when a REST endpoint processing would write a bunch of DomainEvents in one go to the DB): we would open a DB TX and start a Kafka TX, then fetch all unsent Domain Events from the DB, mark them as sent, emit them via Kafka, commit the Kafka TX and then the DB TX. Now there is still one problem with this approach: what if the emitting of the Domain Events succeeds (whether via Kafka transactions or not doesn't matter) but comitting the DB tx fails (for whatever reason)? Then we have the problem that the Domain Events are emitted but the DB state does not reflect this. The only reasonable thing to do is to redo the whole Domain Event processing after we recovered from failure (e.g. after a restart or fixing the infrastructure) - which now means and drives the point home: we would emit the Domain Events a second time, therefore arriving at *at-least-once* delivery, which is the ultimate reason why we need a deduplication mechanism in the processing service (the fixtures services). An interesting problem here is to decide which service *instance* emits which Domain Events? Given that we potentially run each microservice with multiple instances, we also have multiple instances of the asynchronous Domain Event processor - however even if we have some deduplication mechanism in place, not every instance should emit each Domain Event, as this is simply an inefficiency we can avoid. A simple solution would be to add an *instance* column to each Domain Event, and only the corresponding instance can emit the Domain Event. Ofc this creates problems when an instance gets shutdown forever, so we need to make sure that there are no "leftovers" left in the *DomainEvents* table. One question you might ask now is why didn't we do all of this when updating the *Assignment* aggregate? The answer is simple: for resiliency and robustness reasons.

At the same time we need to have something like a *DomainEvent_inbox* table that stores the Domain Events we have processed. This way we candetect duplicates, and also have a history of all processed Domain Events, based on the actual service where it was processed. One might think that when we employ Kafka, which also stores messages, that this creates an unncessary overhead. Yes, it does, but it is a necessary one, because the two things are fundamentally different: Kafka constitutes an event **stream** which is processed sequentially and can not be accessed randomly, the stored Domain Events however constitutes **state** which can be accessed randomly - both serve fundamentally different purposes (also don't forget that you probably won't keep Kafkas messages around forever but will define some retention policy). Also, the history of Domain Events might be needed for debugging and monitoring purposes and also relevant for auditing and even in terms of business rules - all of which is not relevant in this simple project, but might be in a more complex one.
Let's examine this a bit more closely: when we receive a Domain Event from Kafka, we potentially transform some entity in the DB via a DB TX and then commit the Kafka offset. Again, we have 2 different transactional / side effect contexts: committing the DB context and committing the Kafka offset and either can go wrong. If we commit the DB Tx but the committing of the Kafka offset fails, then we might end up processing the same event twice, therefore we store the Domain Event in the *DomainEvent_inbox* table during the DB TX. If now the Kafka committing fails, we simply retry but we see that we already processed the event and can simply skip it. Dealing with the transactional/side-effect boundaries the other way round would not work: if we commit the Kafka offset and then the DB TX but the latter fails, we have the problem that we would lose the Domain Event, therefore we need to do it the other way round.

Consequences of eventual completion / making things resilient:
- TODO: read microservices patterns book
- Apparently we need to have some form of idempotency - why? and how do we implement it?
- how do we deal with re-ordering? can we map this to business semantics?
- how do we bring in sending of the message into the DB transaction? by storing the message to send in the DB and then sending it via a separate, asynchronous message processor, that gets triggered by some notification from PG as soon as there is a new entry.
- how do we deal with the case that a refereree is already assigned to a fixture when assigning a new one? this has to be managed via business semantics/a proper business workflow: we introduced a "pending" state for referee assignments, that is then switched to committed when the fixture services confirms the actual assignment - or its reverted if the assignment in the fixture service failed due to business violation (e.g. referee is already assigned to another fixture or referee is not available for the new fixture or there is another referee already assigned to this fixture).
